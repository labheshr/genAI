{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             precision_recall_fscore_support,\n",
    "                             confusion_matrix, \n",
    "                             classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small dataset for financial sentiment (positive , negative, neutral)\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"bert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model parameters: 109,484,547\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           num_labels=3) \n",
    "# 3 labels = positive, negative, neutral\n",
    "print(f\"Base model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ed7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = pipeline(\n",
    "#         \"text-classification\",\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         top_k=None\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89732d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2264 sentences\n",
      "Label distribution: [ 303 1391  570]\n"
     ]
    }
   ],
   "source": [
    "# explore data and assign test & train labels\n",
    "import numpy as np\n",
    "# taking arbitrary datasets so it has one of each label type\n",
    "test_data = dataset[\"train\"]  # FinancialPhraseBank only has train split\n",
    "sentences = test_data[\"sentence\"]\n",
    "true_labels = test_data[\"label\"]\n",
    "\n",
    "print(f\"Dataset size: {len(sentences)} sentences\")\n",
    "print(f\"Label distribution: {np.bincount(true_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea5e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Add zero_division parameter to handle undefined metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4bf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the sentences\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37707231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e214d2cc294a45bd67050967ac7a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='283' max='283' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [283/283 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5777\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "eval_data = {\n",
    "    \"sentence\": list(sentences),  # Ensure it's a proper list\n",
    "    \"label\": list(true_labels)    # Ensure it's a proper list\n",
    "}\n",
    "\n",
    "\n",
    "eval_dataset = Dataset.from_dict(eval_data)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# IMPORTANT: Remove the original text column and rename label to labels\n",
    "eval_dataset = eval_dataset.remove_columns(['sentence'])\n",
    "eval_dataset = eval_dataset.rename_column('label', 'labels')\n",
    "\n",
    "# Set up trainer for evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./temp_eval',\n",
    "    per_device_eval_batch_size=8,  # Small batch size for your 10 examples\n",
    "    dataloader_drop_last=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_model, \n",
    "    LoraConfig, \n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbc2d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense', 'value', 'key', 'classifier', 'query']\n"
     ]
    }
   ],
   "source": [
    "# finr the target modules for our model (bert-base-uncased)\n",
    "import torch\n",
    "linear_cls = torch.nn.Linear\n",
    "names = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, linear_cls):\n",
    "        names.append(name.split('.')[-1])\n",
    "print(list(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa96f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"query\",\n",
    "        \"value\", \n",
    "        \"key\",\n",
    "        \"dense\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b9f0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model = peft_model.float()  # Ensure float32\n",
    "peft_model = peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe337c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters: 110,826,246\n",
      "Trainable parameters: 1,344,006\n",
      "Percentage of trainable parameters: 1.21%\n"
     ]
    }
   ],
   "source": [
    "print(f\"PEFT model parameters: {peft_model.num_parameters():,}\")\n",
    "trainable_params = peft_model.get_nb_trainable_parameters()\n",
    "trainable_params = trainable_params[0]  # Extract the number from tuple\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params / peft_model.num_parameters() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "308f7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer, test_size=0.2):\n",
    "    dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)\n",
    "    train_val_split = dataset[\"train\"].train_test_split(test_size=test_size, seed=42)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"sentence\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',  # We'll pad dynamically in the data collator\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    # Tokenize datasets\n",
    "    tokenized_train = train_val_split[\"train\"].map(tokenize_function, batched=True)\n",
    "    tokenized_val = train_val_split[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "    print(f\"Training samples: {len(tokenized_train)}\")\n",
    "    print(f\"Validation samples: {len(tokenized_val)}\")\n",
    "\n",
    "    # Check label distribution\n",
    "    train_labels = tokenized_train[\"label\"]\n",
    "    print(f\"Label distribution: {np.bincount(train_labels)}\")\n",
    "\n",
    "    return tokenized_train, tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abc1f4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1811\n",
      "Validation samples: 453\n",
      "Label distribution: [ 230 1111  470]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = prepare_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db9e98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    #max_length=512,\n",
    "    pad_to_multiple_of=8,  # For efficiency on GPU\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5186413",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # empty cache\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./peft_financial_sentiment_fixed\",\n",
    "    num_train_epochs=2,  # Reduced epochs\n",
    "    per_device_train_batch_size=4,  # Smaller batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=50,  # Reduced warmup\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=3e-4,  # IMPORTANT: Add explicit learning rate\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,  # More frequent evaluation\n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    gradient_checkpointing=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=False,\n",
    "    group_by_length=False,\n",
    "    fp16=False,  # Explicitly disable mixed precision\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d62d6364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='906' max='906' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [906/906 07:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>0.540271</td>\n",
       "      <td>0.754967</td>\n",
       "      <td>0.717461</td>\n",
       "      <td>0.798167</td>\n",
       "      <td>0.754967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.434057</td>\n",
       "      <td>0.777042</td>\n",
       "      <td>0.727658</td>\n",
       "      <td>0.702129</td>\n",
       "      <td>0.777042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.305738</td>\n",
       "      <td>0.931567</td>\n",
       "      <td>0.929712</td>\n",
       "      <td>0.933842</td>\n",
       "      <td>0.931567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.426732</td>\n",
       "      <td>0.898455</td>\n",
       "      <td>0.901859</td>\n",
       "      <td>0.916757</td>\n",
       "      <td>0.898455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.201719</td>\n",
       "      <td>0.955850</td>\n",
       "      <td>0.955711</td>\n",
       "      <td>0.955628</td>\n",
       "      <td>0.955850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.246559</td>\n",
       "      <td>0.951435</td>\n",
       "      <td>0.950838</td>\n",
       "      <td>0.950947</td>\n",
       "      <td>0.951435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.147053</td>\n",
       "      <td>0.971302</td>\n",
       "      <td>0.971251</td>\n",
       "      <td>0.971215</td>\n",
       "      <td>0.971302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.171835</td>\n",
       "      <td>0.969095</td>\n",
       "      <td>0.969014</td>\n",
       "      <td>0.969554</td>\n",
       "      <td>0.969095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.161317</td>\n",
       "      <td>0.969095</td>\n",
       "      <td>0.969124</td>\n",
       "      <td>0.969641</td>\n",
       "      <td>0.969095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 0.2805\n",
      "Training steps: 906\n",
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  eval_loss: 0.1471\n",
      "  eval_accuracy: 0.9713\n",
      "  eval_f1: 0.9713\n",
      "  eval_precision: 0.9712\n",
      "  eval_recall: 0.9713\n",
      "  eval_runtime: 16.4393\n",
      "  eval_samples_per_second: 27.5560\n",
      "  eval_steps_per_second: 3.4670\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training results\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training steps: {train_result.global_step}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in eval_result.items():\n",
    "    if key.startswith('eval_'):\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af2fd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "peft_model.save_pretrained(\"/tmp/my_finsentiment_pretrained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "peft_model_saved = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"/tmp/my_finsentiment_pretrained_model\",\n",
    "    num_labels=3  # Explicitly specify 3 classes for financial sentiment\n",
    ")\n",
    "peft_model_saved = peft_model_saved.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc2c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PEFT model...\n",
      "Evaluating PEFT model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='283' max='283' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [283/283 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PEFT Model Results:\n",
      "Accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating PEFT model...\")\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir='./temp_peft_eval',\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_drop_last=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model_saved,\n",
    "    args=peft_training_args,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the PEFT model\n",
    "print(\"Evaluating PEFT model...\")\n",
    "peft_eval_results = peft_trainer.evaluate()\n",
    "\n",
    "# Print PEFT model results\n",
    "print(f\"\\nPEFT Model Results:\")\n",
    "print(f\"Accuracy: {peft_eval_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Before: 0.5777385159010601\n",
      "accuracy_after=0.9784\n",
      "Improvement in accuracy=69.34250764525993%\n"
     ]
    }
   ],
   "source": [
    "accuracy_before = eval_results['eval_accuracy']\n",
    "accuracy_after = peft_eval_results['eval_accuracy']\n",
    "print(f\"Accuracy Before: {accuracy_before}\")\n",
    "print(f\"accuracy_after={accuracy_after:.4f}\")\n",
    "print(f\"Improvement in accuracy={(accuracy_after/accuracy_before-1)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca346e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
